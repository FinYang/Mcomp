---
title: "Introduction to the Mcomp package"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Introduction to the Mcomp package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center" #,
  # fig.width = 4,
  # fig.height = 2
)
library(knitr)
```

The R package *Mcomp* contains the 1001 time series from the M-competition (Makridakis et al. 1982) and the 3003 time series from the IJF-M3 competition (Makridakis and Hibon, 2000). See also the [tscompdata package](https://github.com/robjhyndman/tscompdata).  

## Installation

You can install the **stable** version on
[R CRAN](https://cran.r-project.org/package=Mcomp).

```r
install.packages('Mcomp')
```

You can install the **development** version from
[Github](https://github.com/robjhyndman/Mcomp)

```r
# install.packages("devtools")
devtools::install_github("robjhyndman/Mcomp")
```

## M1 competition

The M1 forecasting competition was organized by Spyros Makridakis and Michèle Hibon, involving 1001 series. In this competition, anyone could submit forecasts, making this the first true forecasting competition as far as I am aware. They also used multiple forecast measures to determine the most accurate method.    

The 1001 time series were taken from demography, industry and economics, and ranged in length between 9 and 132 observations. All the data were either non-seasonal (e.g., annual), quarterly or monthly. Curiously, all the data were positive, which made it possibly to compute mean absolute percentage errors, but was not really reflective of the population of real data.  

The M1 competition data are stored in the set `M1` of class `Mcomp`.  

```{r}
library(Mcomp)
M1
```

Functions in `Mcomp` package help to display and manage the date. `plot.Mdata` and `autoplot.Mdata` functions plot a time series, showing both the training and test sections of the series. `autoplot.Mdata` returns a ggplot2 object while `plot.Mdata` returns nothing.  

```{r}
class(plot(M1[[1]]))
```
```{r  fig.width = 5,  fig.height = 3}
autoplot(M1[[1]])
class(autoplot(M1[[1]]))
```

`subset.Mcomp` returns a subset of the time series data from the M Competitions. Subsets can be for specific periods, or specific types of data or both.  

```{r}
subset(M1,"monthly")
subset(M1,"macro1")

```

The 111 series used in the extended comparisons in the 1982 M-competition can be selected.  

```{r}
subset(M1,111)
```



## M3 competition

In 1998, Makridakis & Hibon ran their M3 competition. Entrants had to forecast 3003 time series and the results were compared to a test set that was withheld from participants. The time series were all taken from business, demography, finance and economics, and ranging in length between 14 and 126 observations. Again, the data were all either non-seasonal (e.g., annual), quarterly or monthly, and all were positive.

The [competition results](http://www.forecastingprinciples.com/paperpdf/Makridakia-The%20M3%20Competition.pdf) are publicly available in an IJF paper published in 2000. The best performing methods overall were the Theta method, ForecastPro and ForecastX, as measured by the symmetric MAPE (sMAPE) that was favoured by Makridakis and Hibon. The following table shows some of the results from the original competition including results from the main commercial software vendors. The first sMAPE column is taken from the original paper. My own recalculation of the sMAPE results usually gives values slightly less than those published (I don’t know why). The MAPE column shows the mean absolute percentage error and the MASE column shows the mean absolute scaled errors.

```{r echo = FALSE}
# read_html("https://robjhyndman.com/hyndsight/show-me-the-evidence/") %>% 
#   html_node("table") %>% html_table() %>% kable()require(Mcomp)
nseries <- length(M3) 
theta <- as.matrix(M3Forecast$THETA) 
fpro <- as.matrix(M3Forecast$ForecastPro) 
fcx <- as.matrix(M3Forecast$ForcX) 
bjauto <- as.matrix(M3Forecast$`B-J auto`) 
ab1 <- as.matrix(M3Forecast$AutoBox1) 
ab2 <- as.matrix(M3Forecast$AutoBox2) 
ab3 <- as.matrix(M3Forecast$AutoBox3) 
ets1 <- aarima <- hybrid <- matrix(NA,nrow=nseries,ncol=18) 
for(i in 1:nseries) { 
  ets1[i,] <- forecast(ets(M3[[i]]$x),h=18,PI=FALSE)$mean 
  aarima[i,] <- forecast(auto.arima(M3[[i]]$x),h=18)$mean 
  hybrid[i,] <- 0.5*(aarima[i,] + ets1[i,]) 
} 
# Compute accuracy 
mase <- mape <- smape <- matrix(NA,nrow=10,ncol=nseries) 
f <- matrix(NA, nrow=10, ncol=18) 
for(i in 1:nseries) { 
  x <- M3[[i]]$xx
  n <- length(x) 
  f[1,1:n] <- theta[i,1:n]
  f[2,1:n] <- fpro[i,1:n] 
  f[3,1:n] <- fcx[i,1:n] 
  f[4,1:n] <- bjauto[i,1:n]
  f[5,1:n] <- ab1[i,1:n] 
  f[6,1:n] <- ab2[i,1:n]
  f[7,1:n] <- ab3[i,1:n]
  f[8,1:n] <- ets1[i,1:n] 
  f[9,1:n] <- aarima[i,1:n]
  f[10,1:n] <- hybrid[i,1:n]
  scale <- mean(abs(diff(M3[[i]]$x, lag=frequency(x)))) 
  for(j in 1:10) {
    mape[j,i] <- mean(abs((x-f[j,1:n])/x))*100
    smape[j,i] <- mean(abs(x-f[j,1:n])/(abs(x)+abs(f[j,1:n])))*200 
    mase[j,i] <- mean(abs(x-f[j,1:n])/scale) 
  } 
} 
```
```{r}
# All series 
m3table <- matrix(NA, nrow=10, ncol=3) 
m3table[,1] <- rowMeans(mape,na.rm=TRUE) 
m3table[,2] <- rowMeans(smape)
m3table[,3] <- rowMeans(mase) 
rownames(m3table) <- c("Theta","ForecastPro","ForecastX","BJauto", "Autobox1","Autobox2","Autobox3", "ETS","AutoARIMA","Hybrid") 
colnames(m3table) <- c("MAPE","Average_sMAPE_recalculated","MASE") 
j <- order(m3table[,3]) 
# round(m3table[j,],2) 
m3table <- cbind(m3table, Average_sMAPE=c(13.01, 13.19, 13.49, 14.01, 15.23, 14.41, 15.33, NA, NA, NA))

kable(round(m3table[c("Theta","ForecastPro","ForecastX","BJauto", "Autobox2","Autobox1","Autobox3") ,c("Average_sMAPE","Average_sMAPE_recalculated", "MAPE", "MASE")],2))
```

BJ automatic was produced by ForecastPro but with the forecasts restricted to ARIMA models. For some reason, Autobox was allowed three separate submissions (a practice normally not allowed as it leads to over-fitting on the test set).  

Any good forecasting software should be aiming to get close to (or better than) Theta on this test. After all, the M3 competition was held more than 15 years ago. Presumably all of the software companies have tried to improve their results since then. Unfortunately, none of them to my knowledge has published any updated figures. I wish they would (preferably independently verified). It would provide some evidence that they are improving their algorithms.  

My aim with the forecast package for R is to make freely available state-of-the-art algorithms for some forecasting models. I do not attempt to offer a comprehensive suite of algorithms, but what I do provide gives forecasts that are in the same ballpark as the best methods in the M3 competition. Here is the evidence.  

```{r echo = FALSE}
# read_html("https://robjhyndman.com/hyndsight/show-me-the-evidence/") %>% 
  # html_node("table:nth-child(8)") %>% html_table() %>% kable()

kable(round(m3table[c( "ETS", "AutoARIMA","Hybrid"),c("Average_sMAPE_recalculated", "MAPE", "MASE")],2))
```

The last method is a simple average of the forecasts from ets and auto.arima. If you only want point forecasts, that is the best approach available in the forecast package. It is also better than any of the commercial software (at least as far as they have been prepared to subject their algorithms to independent testing).  

Unlike the commercial vendors, you don’t have to take my word for it. My algorithms are open source, and the [code that produced the above tables can be downloaded](https://robjhyndman.com/Rfiles/m3comparisons.R).  

The time series from the M3 forecasting competition and the forecasts from all the original participating methods are stored in `M3` and `M3Forecast` respectively. `plot.Mdata`, `autoplot.Mdata` and `subset.Mcomp` work on `M3` as well. `M3Forecast` is a list of data.frames. More details on `M3Forecast` can be found in help documentation.

```{r  fig.width = 5,  fig.height = 3}
M3
autoplot(M3[[1]])
subset(M3, "macro")
```




## Sources

[Makridakis, S., A. Andersen, R. Carbone, R. Fildes, M. Hibon, R. Lewandowski, J. Newton, E. Parzen, and R. Winkler (1982) The accuracy of extrapolation (time series) methods: results of a forecasting competition. *Journal of Forecasting*, **1**, 111--153.](http://doi.org/10.1002/for.3980010202)

[Makridakis and Hibon (2000) The M3-competition: results, conclusions and implications. *International Journal of Forecasting*, **16**, 451-476.](https://doi.org/10.1016/S0169-2070(00)00057-1)

## License

This package is free and open source software, licensed under GPL-3
